{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_results(pca, df):\n",
    "    dimensions = ['dimention {}'.format(i) for i in range(1,pca.n_components_+1)]\n",
    "    fig, ax = plt.subplots(figsize=(18,12))\n",
    "    components = pd.DataFrame(pca.components_)\n",
    "    components.plot(ax=ax, kind='bar');\n",
    "    labels = [str(s) for s in df.columns]\n",
    "    ax.legend(labels)\n",
    "    ax.set_ylabel('Feature Weights')\n",
    "    ax.set_xticklabels(dimensions, rotation=90)\n",
    "    for i, ev in enumerate(np.round(pca.explained_variance_ratio_, 3)):\n",
    "        ax.text(i-0.04, ax.get_ylim()[1]+0.05, ev)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_2d_plot(pca, df):\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    transformed_data = pca.transform(df.values)\n",
    "    ax.scatter(transformed_data[:,0], transformed_data[:,1], s=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependant_variable_detector(df):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    scaler = StandardScaler()\n",
    "    lr = LinearRegression()\n",
    "    columns = list(df.columns)\n",
    "    for col in columns:\n",
    "        y = scaler.fit_transform(df[col].values.reshape(-1,1))\n",
    "        X = scaler.fit_transform(df.drop(col, axis=1).values)\n",
    "        lr.fit(X,y)\n",
    "        print('Using '+col+' as dependent variable R2 score is :'+str(lr.score(X,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_matrix(df):\n",
    "    df_corr = df.corr()\n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    cax = ax.matshow(df_corr.values, interpolation='nearest')\n",
    "    fig.colorbar(cax)\n",
    "    plt.xticks(range(len(df.columns)), df.columns)\n",
    "    plt.yticks(range(len(df.columns)), df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turkey_outlier_detector(df, cols=None):\n",
    "    if cols  is None:\n",
    "        cols = [str(s) for s in df.describe().columns]\n",
    "        \n",
    "    q1 = {}\n",
    "    q3 = {}\n",
    "    iqd = {}\n",
    "    r_limit = {}\n",
    "    l_limit = {}\n",
    "    outlier_count = {}\n",
    "    outlier_indices = {}\n",
    "    for col in cols:\n",
    "        q1[col] = np.percentile(df[col].values, 25)\n",
    "        q3[col] = np.percentile(df[col].values, 75)\n",
    "        iqd[col] = q3[col] - q1[col]\n",
    "        r_limit[col] = q3[col] + 1.5*iqd[col]\n",
    "        l_limit[col] = q1[col] - 1.5*iqd[col]\n",
    "        data_outlier = df[~((df[col]<r_limit[col]).multiply(df[col]>l_limit[col]))]\n",
    "        outlier_count[col] = data_outlier.shape[0]\n",
    "        outlier_indices[col] = data_outlier.index\n",
    "        \n",
    "    for col in cols:\n",
    "        print('_'*25)\n",
    "        print(col+'-'*8+'>'+str(outlier_count[col]))\n",
    "        \n",
    "    return outlier_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hopkins_statistic(df):\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    n_samples = df.shape[0]\n",
    "    num_samples = [int(f*n_samples) for f in [0.25,0.5,0.75]]\n",
    "    states = [123,42,67,248,654]\n",
    "    for n in num_samples:\n",
    "        print('-'*12+str(n)+'-'*12)\n",
    "        hopkins_statistic = []\n",
    "        for random_state in states:\n",
    "            data = df.sample(n=n, random_state=random_state)\n",
    "            nbrs = NearestNeighbors(n_neighbors=2)\n",
    "            scaler = StandardScaler()\n",
    "            X = scaler.fit_transform(data.values)\n",
    "            nbrs.fit(X)\n",
    "            sample_dist = nbrs.kneighbors(X)[0][:,1]\n",
    "            sample_dist = np.sum(sample_dist)\n",
    "            random_data = np.random.rand(X.shape[0], X.shape[1])\n",
    "            nbrs.fit(random_data)\n",
    "            random_dist = nbrs.kneighbors(random_data)[0][:,1]\n",
    "            random_dist = np.sum(random_dist)\n",
    "            hs = sample_dist/(sample_dist+random_dist)\n",
    "            hopkins_statistic.append(hs)\n",
    "            print('*'*25)\n",
    "            print('hopkins statistic :'+str(hs))\n",
    "        print('mean hopkins statistic :'+str(np.mean(np.array(hopkins_statistic))))\n",
    "        print('hopkins statistic standard deviation :'+str(np.std(np.array(hopkins_statistic))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kth_nearest_data_point(df, k_max):\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    ks = range(1,k_max+1)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(df.values)\n",
    "    nbrs = NearestNeighbors(n_neighbors=k_max)\n",
    "    nbrs.fit(X)\n",
    "    kneighbors_result = nbrs.kneighbors()[0]\n",
    "    kth_neighbor_dist = list(np.sum(kneighbors_result, axis=0))\n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    ax.plot(ks, kth_neighbor_dist);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhoutte_score(X, max_clusters=20):\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    num_clusters = range(2,max_clusters+1)\n",
    "    sil_score = []\n",
    "    for n in num_clusters:\n",
    "        kmeans = KMeans(n_clusters=n)\n",
    "        kmeans.fit(X)\n",
    "        preds = kmeans.predict(X)\n",
    "        sil_score.append(silhouette_score(X, preds))\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    ax.plot(num_clusters, sil_score)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_partition_measure(X, k_max):\n",
    "    from sklearn.cluster import KMeans\n",
    "    ks = range(1,k_max+1)\n",
    "    UPM = []\n",
    "    for k in ks:\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(X)\n",
    "        UPM.append(kmeans.inertia_)\n",
    "    fig, ax = plt.subplots(figsize=(14,14))\n",
    "    ax.plot(ks, UPM);\n",
    "    plt.show()\n",
    "    return UPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_partition_measure(X, k_max):\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics.pairwise import  pairwise_distances\n",
    "    ks = range(1,k_max+1)\n",
    "    OPM = []\n",
    "    for k in ks:\n",
    "        kmeans = KMeans(n_clusters=k)\n",
    "        kmeans.fit(X)\n",
    "        centers = kmeans.cluster_centers_\n",
    "        d_min = np.inf\n",
    "        for pair in list(itertools.combinations(centers, 2)):\n",
    "            d = pairwise_distances(pair[0].reshape(1,-1), pair[1].reshape(1,-1), metric='euclidean')\n",
    "            if d<d_min:\n",
    "                d_min = d\n",
    "        OPM.append(k/d_min)\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    ax.plot(ks, OPM)\n",
    "    plt.show()\n",
    "    return OPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validity_index(X, k_max):\n",
    "    UPM = under_partition_measure(X, k_max)\n",
    "    OPM = over_partition_measure(X, k_max)\n",
    "    UPM_min = np.min(UPM)\n",
    "    OPM_min = np.min(OPM)\n",
    "    UPM_max = np.max(UPM)\n",
    "    OPM_max = np.max(OPM)\n",
    "    norm_UPM = []\n",
    "    norm_OPM = []\n",
    "    for i in range(k_max):\n",
    "        norm_UPM.append((UPM[i]-UPM_min)/(UPM_max-UPM_min))\n",
    "        norm_OPM.append((OPM[i]-OPM_min)/(OPM_max-OPM_min))\n",
    "        \n",
    "    validity_index = np.array(norm_UPM)+np.array(norm_OPM)\n",
    "    fig, ax = plt.subplots(figsize=(12,12))\n",
    "    ax.plot(range(1,k_max+1), validity_index)\n",
    "    return validity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
